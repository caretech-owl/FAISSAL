{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "def load_llm(model_type, model_path):\n",
    "        if model_type == \"LLaMA-7B\":\n",
    "                llm = Llama(model_path= model_path, \n",
    "                n_ctx= 2048,\n",
    "                n_parts= -1,\n",
    "                n_gpu_layers = 22, #16\n",
    "                n_threads = 6, #4\n",
    "                n_batch= 128, #128\n",
    "                last_n_tokens_size = 64\n",
    "                )\n",
    "        return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_llm(llm, question, context, context_dependency):\n",
    "    if context_dependency == \"low\":\n",
    "        output = llm(f\"### Instruction: Answer the question. ### Question: {question} ### Context: {context} ### Answer:\",\n",
    "            temperature= 0.75, top_k = 10000, repeat_penalty = 1.1, max_tokens = 2048, echo=True) # for 7B\n",
    "    elif context_dependency == \"high\":\n",
    "        output = llm(f\"### Instruction: Answer the question based only on the provided context, if the information is not provided in the context you must answer the question by saying that the required information cannot be found in the context. ### Question: {question} ### Context: {context} ### Answer:\",\n",
    "                     temperature= 0.25, top_k = 100, repeat_penalty = 1.1, max_tokens = 2048, echo=False) # for 13B\n",
    "    elif context_dependency == \"medium\":\n",
    "        output = llm(f\"### Instruction: Answer the question based only on the provided context, you are allowed to use external information as a last resort, if and only if the the required information cannot be found in the context. ### Question: {question} ### Context: {context} ### Answer:\",\n",
    "                     temperature= 0.5, top_k = 100, repeat_penalty = 1.1, max_tokens = 2048, echo=False) # experimental prompt\n",
    "    else:\n",
    "        raise ValueError(\"Invalid prompt style: {}\".format(context_dependency))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = load_llm(\"LLaMA-7B\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = run_llm(llm, \"Write a one-paragraph essay about World War I\", None, \"low\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(output[\"choices\"][0][\"text\"].split(\"###\")[4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
